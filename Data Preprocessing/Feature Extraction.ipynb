{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0990a7d",
   "metadata": {},
   "source": [
    "</br>\n",
    "<h1 align=\"center\" style=\"color:green\">Feature Extraction on Meteorological & Dengue Dataset in the City of Manila</br> DOST-Pagasa Port Area Dataset & DOH City of Manila Dengue Dataset</h1>\n",
    "<div style=\"text-align:center\">Prepared by <b>Jose Rafael C Crisostomo, Jan Vincent G. Elleazar, Dodge Deiniol D. Lapis, and Carl Jacob F. Mateo</b><br>\n",
    "FOMaC-Autoformer: A Hybrid First Order Markov Chain-Autoformer Model for Dengue Incidence Forecasting in the City of Manila<br>\n",
    "<b>University of Santo Tomas - College of Information and Computing Sciences</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ad6d86",
   "metadata": {},
   "source": [
    "We will extract all features from both datasets into the following: Temporal Features, Statistical Features, and Composite Features. These will then converted into feature vectors in which allows the Autoformer model to be fed by the right input data.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e1cb1c",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a80ff78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loaded Data ---\n",
      "               location  cases  cases_minmax  RAINFALL_minmax  TMAX_minmax  \\\n",
      "date                                                                         \n",
      "2016-01-10  MANILA CITY     49      0.204167         0.005472     0.380835   \n",
      "2016-01-17  MANILA CITY     47      0.195833         0.008208     0.471744   \n",
      "2016-01-24  MANILA CITY     37      0.154167         0.008208     0.455774   \n",
      "2016-01-31  MANILA CITY     31      0.129167         0.008208     0.380835   \n",
      "2016-02-07  MANILA CITY     33      0.137500         0.167715     0.253071   \n",
      "\n",
      "            TMIN_minmax  RH_minmax  WIND_SPEED_minmax  WIND_DIR_X  WIND_DIR_Y  \\\n",
      "date                                                                            \n",
      "2016-01-10     0.298276   0.500000           0.225734   -0.481295   -0.876559   \n",
      "2016-01-17     0.443103   0.511666           0.322799   -0.229200   -0.973379   \n",
      "2016-01-24     0.460345   0.403147           0.354402   -0.844328   -0.535827   \n",
      "2016-01-31     0.362069   0.387412           0.354402   -0.222229    0.974994   \n",
      "2016-02-07     0.324138   0.620184           0.259594    0.010472   -0.999945   \n",
      "\n",
      "             state  \n",
      "date                \n",
      "2016-01-10  Medium  \n",
      "2016-01-17  Medium  \n",
      "2016-01-24  Medium  \n",
      "2016-01-31     Low  \n",
      "2016-02-07     Low  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset you created in the state discretization step\n",
    "try:\n",
    "    data = pd.read_csv('final_data_with_states.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'final_data_with_states.csv' not found.\")\n",
    "    print(\"Please run your state discretization script first.\")\n",
    "    # In a real script, you'd exit here\n",
    "\n",
    "# Set 'date' as the index. This is VITAL for time-series features.\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data.set_index('date', inplace=True)\n",
    "data.sort_index(inplace=True)\n",
    "\n",
    "print(\"--- Loaded Data ---\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807619d0",
   "metadata": {},
   "source": [
    "# Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d96bdfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1. Creating Temporal Features ---\n",
      "Temporal features created and encoded.\n",
      "            month_sin  month_cos  week_of_year_sin  week_of_year_cos\n",
      "date                                                                \n",
      "2016-01-10   0.500000   0.866025          0.120537          0.992709\n",
      "2016-01-17   0.500000   0.866025          0.239316          0.970942\n",
      "2016-01-24   0.500000   0.866025          0.354605          0.935016\n",
      "2016-01-31   0.500000   0.866025          0.464723          0.885456\n",
      "2016-02-07   0.866025   0.500000          0.568065          0.822984\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 1. Creating Temporal Features ---\")\n",
    "\n",
    "# Extract basic time features\n",
    "data['month'] = data.index.month\n",
    "data['week_of_year'] = data.index.isocalendar().week\n",
    "data['day_of_year'] = data.index.dayofyear\n",
    "data['year'] = data.index.year\n",
    "\n",
    "# --- Cyclical Feature Encoding ---\n",
    "# This is a best practice. It helps the model understand that\n",
    "# December (12) is \"close\" to January (1).\n",
    "\n",
    "def encode_cyclical(df, col, max_val):\n",
    "    df[col + '_sin'] = np.sin(2 * np.pi * df[col] / max_val)\n",
    "    df[col + '_cos'] = np.cos(2 * np.pi * df[col] / max_val)\n",
    "    return df\n",
    "\n",
    "# Encode month and week_of_year\n",
    "data = encode_cyclical(data, 'month', 12)\n",
    "data = encode_cyclical(data, 'week_of_year', 52)\n",
    "\n",
    "# Now we can drop the original month/week columns if we want\n",
    "# data.drop(['month', 'week_of_year'], axis=1, inplace=True)\n",
    "\n",
    "print(\"Temporal features created and encoded.\")\n",
    "print(data[['month_sin', 'month_cos', 'week_of_year_sin', 'week_of_year_cos']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82446a38",
   "metadata": {},
   "source": [
    "***\n",
    "# Statistical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0338de71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Creating Statistical Features (Lags & Rolling) ---\n",
      "Statistical features created.\n",
      "            RAINFALL_minmax  RAINFALL_minmax_roll_mean_4w  \\\n",
      "date                                                        \n",
      "2016-01-10         0.005472                           NaN   \n",
      "2016-01-17         0.008208                           NaN   \n",
      "2016-01-24         0.008208                           NaN   \n",
      "2016-01-31         0.008208                      0.007524   \n",
      "2016-02-07         0.167715                      0.048085   \n",
      "2016-02-14         0.008208                      0.048085   \n",
      "2016-02-21         0.008208                      0.048085   \n",
      "2016-02-28         0.018057                      0.050547   \n",
      "2016-03-06         0.005472                      0.009986   \n",
      "2016-03-13         0.008208                      0.009986   \n",
      "\n",
      "            RAINFALL_minmax_lag_1w  RAINFALL_minmax_lag_2w  \\\n",
      "date                                                         \n",
      "2016-01-10                     NaN                     NaN   \n",
      "2016-01-17                0.005472                     NaN   \n",
      "2016-01-24                0.008208                0.005472   \n",
      "2016-01-31                0.008208                0.008208   \n",
      "2016-02-07                0.008208                0.008208   \n",
      "2016-02-14                0.167715                0.008208   \n",
      "2016-02-21                0.008208                0.167715   \n",
      "2016-02-28                0.008208                0.008208   \n",
      "2016-03-06                0.018057                0.008208   \n",
      "2016-03-13                0.005472                0.018057   \n",
      "\n",
      "            RAINFALL_minmax_lag_3w  RAINFALL_minmax_lag_4w  \n",
      "date                                                        \n",
      "2016-01-10                     NaN                     NaN  \n",
      "2016-01-17                     NaN                     NaN  \n",
      "2016-01-24                     NaN                     NaN  \n",
      "2016-01-31                0.005472                     NaN  \n",
      "2016-02-07                0.008208                0.005472  \n",
      "2016-02-14                0.008208                0.008208  \n",
      "2016-02-21                0.008208                0.008208  \n",
      "2016-02-28                0.167715                0.008208  \n",
      "2016-03-06                0.008208                0.167715  \n",
      "2016-03-13                0.008208                0.008208  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 2. Creating Statistical Features (Lags & Rolling) ---\")\n",
    "\n",
    "# List of columns we want to create features for\n",
    "# We use the minmax-scaled features for this\n",
    "features_to_lag = [\n",
    "    'RAINFALL_minmax', \n",
    "    'TMAX_minmax', \n",
    "    'TMIN_minmax', \n",
    "    'RH_minmax',\n",
    "    'cases_minmax'  # Lagging the target itself is a key predictive feature\n",
    "]\n",
    "\n",
    "# Define our window sizes\n",
    "# We'll use a 4-week window for rolling averages\n",
    "window_size = 4\n",
    "# We'll create lags for 1, 2, 3, and 4 weeks\n",
    "lag_periods = [1, 2, 3, 4]\n",
    "\n",
    "for col in features_to_lag:\n",
    "    # --- Rolling Averages ---\n",
    "    # Calculates the mean of the last 'window_size' weeks\n",
    "    col_roll_mean = f'{col}_roll_mean_{window_size}w'\n",
    "    data[col_roll_mean] = data[col].rolling(window=window_size).mean()\n",
    "    \n",
    "    # --- Lagged Features ---\n",
    "    # Shows the value from 'n' weeks ago\n",
    "    for lag in lag_periods:\n",
    "        col_lag = f'{col}_lag_{lag}w'\n",
    "        data[col_lag] = data[col].shift(lag)\n",
    "\n",
    "print(\"Statistical features created.\")\n",
    "print(data.filter(like='RAINFALL_minmax').head(10)) # Show first 10 to see NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956fbb49",
   "metadata": {},
   "source": [
    "***\n",
    "# Composite Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0e18997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Creating Composite Features ---\n",
      "Composite features created.\n",
      "            TMAX_minmax  RH_minmax  TMAX_x_RH   T_range\n",
      "date                                                   \n",
      "2016-01-10     0.380835   0.500000   0.190418  0.082560\n",
      "2016-01-17     0.471744   0.511666   0.241375  0.028641\n",
      "2016-01-24     0.455774   0.403147   0.183744 -0.004571\n",
      "2016-01-31     0.380835   0.387412   0.147540  0.018766\n",
      "2016-02-07     0.253071   0.620184   0.156951 -0.071067\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 3. Creating Composite Features ---\")\n",
    "\n",
    "# Interaction between Temperature and Humidity\n",
    "# This \"heat-humidity\" feature could be more predictive than either alone.\n",
    "data['TMAX_x_RH'] = data['TMAX_minmax'] * data['RH_minmax']\n",
    "\n",
    "# Temperature Range\n",
    "data['T_range'] = data['TMAX_minmax'] - data['TMIN_minmax']\n",
    "\n",
    "print(\"Composite features created.\")\n",
    "print(data[['TMAX_minmax', 'RH_minmax', 'TMAX_x_RH', 'T_range']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9595f6",
   "metadata": {},
   "source": [
    "# Clean and save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6a9ccef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Final Cleanup and Save ---\n",
      "Rows with NaN values before dropping: 11\n",
      "Rows with NaN values after dropping: 0\n",
      "Original data shape: (259, 46)\n",
      "Final data shape: (248, 46)\n",
      "\n",
      "--- Feature Engineering Complete! ---\n",
      "Your final dataset 'feature_engineered_data.csv' is ready.\n",
      "This file contains the 'Feature Vectors' for your Autoformer.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 4. Final Cleanup and Save ---\")\n",
    "\n",
    "# Check how many rows have NaN values\n",
    "nan_rows_before = data.isna().any(axis=1).sum()\n",
    "print(f\"Rows with NaN values before dropping: {nan_rows_before}\")\n",
    "\n",
    "# Drop all rows that have ANY NaN values\n",
    "# This ensures our model only trains on complete, valid data\n",
    "final_feature_data = data.dropna()\n",
    "\n",
    "nan_rows_after = final_feature_data.isna().any(axis=1).sum()\n",
    "print(f\"Rows with NaN values after dropping: {nan_rows_after}\")\n",
    "print(f\"Original data shape: {data.shape}\")\n",
    "print(f\"Final data shape: {final_feature_data.shape}\")\n",
    "\n",
    "# Save the final dataset\n",
    "final_feature_data.to_csv('feature_engineered_data.csv')\n",
    "\n",
    "print(\"\\n--- Feature Engineering Complete! ---\")\n",
    "print(\"Your final dataset 'feature_engineered_data.csv' is ready.\")\n",
    "print(\"This file contains the 'Feature Vectors' for your Autoformer.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
